{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT, READ_CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard imports\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "#SKlearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "\n",
    "#NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/javm/Desktop/Projects/Book-Recommendations/books_with_blurbs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT COMPILATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']= df['Title'] + \" \" + df ['Author'] + \" \" + df['Publisher'] + \" \" + df['Blurb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Decision in Normandy Carlo D'Este HarperPerenn...\n",
       "1        Flu: The Story of the Great Influenza Pandemic...\n",
       "2        The Kitchen God's Wife Amy Tan Putnam Pub Grou...\n",
       "3        What If?: The World's Foremost Military Histor...\n",
       "4        Goodbye to the Buttermilk Sky Julia Oliver Riv...\n",
       "                               ...                        \n",
       "57505    Tainted Trail Wen Spencer Roc Ukiah Oregon, ha...\n",
       "57506    Twelve Mile Limit Randy Wayne White Penguin Pu...\n",
       "57507    The Man With the Red Tattoo (James Bond 007) R...\n",
       "57508    Iron Fist (Star Wars: X-Wing Series, Book 6) A...\n",
       "57509    The Adventures of Lando Calrissian: Lando Calr...\n",
       "Name: text, Length: 57510, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function used to remove stopwords\n",
    "def remove_stop_words(sentence):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function used to remove punctuation\n",
    "def remove_punkt(text):\n",
    "    punctuation = string.punctuation\n",
    "    \n",
    "    no_punct = \"\"\n",
    "\n",
    "    for char in text:\n",
    "        if char not in punctuation:\n",
    "            no_punct = no_punct + char\n",
    "\n",
    "    return no_punct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function used to remove non alphabetical characters\n",
    "def remove_non_word_characters(text):\n",
    "    # Use a regular expression pattern to match any non-word character (including punctuation and whitespace)\n",
    "    pattern = r'\\W+'\n",
    "\n",
    "    # Use the re.sub() function to remove all matches of the pattern from the input text\n",
    "    no_non_word = re.sub(pattern, ' ', text)\n",
    "\n",
    "    # Return the text with all non-word characters removed\n",
    "    return no_non_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lowercase all text\n",
    "def lower_all (text):\n",
    "    lowercase = text.lower()\n",
    "    return lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function used to lemmatize\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Use the lemmatizer to lemmatize each word in the input text\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in text.split()]\n",
    "\n",
    "    # Join the lemmatized words back into a single string\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "\n",
    "    # Return the lemmatized text\n",
    "    return lemmatized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function used to keep only alphabetical characters\n",
    "def keep_alphabetical(text):\n",
    "    # Use a regular expression pattern to match any non-alphabetic character (including numbers and punctuation)\n",
    "    pattern = r'[^a-zA-Z]+'\n",
    "\n",
    "    # Use the re.sub() function to remove all matches of the pattern from the input text\n",
    "    alphabetical = re.sub(pattern, ' ', text)\n",
    "\n",
    "    # Return the text with all non-alphabetic characters removed\n",
    "    return alphabetical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to keep only English language words\n",
    "english_words = set(words.words())\n",
    "\n",
    "def extract_english_words(text):\n",
    "    words = text.split()\n",
    "    english_words_only = (word for word in words if word.lower() in english_words)\n",
    "    return english_words_only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sentence):\n",
    "    sentence = remove_stop_words(sentence)\n",
    "    sentence = remove_punkt(sentence)\n",
    "    sentence = remove_non_word_characters(sentence)\n",
    "    sentence = lower_all(sentence)\n",
    "    sentence = lemmatize_text(sentence)\n",
    "    sentence = keep_alphabetical(sentence)\n",
    "    #sentence = extract_english_words(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'haveli laurel leaf book suzanne fisher staple laurel leaf world newbery honor book shabanu vividly recreated novel young pakistani woman s heartbreaking struggle tyranny custom ancient law shabanu mother face daily challenge position husband s household even plan young daughter s education uncertain future visit haveli home city lahore shabanu fall love omar spite tradition forbid union'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vec = TfidfVectorizer(min_df = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = tf_vec.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_df = pd.DataFrame(vectors.toarray(),\n",
    "                     columns=[k for k, v in sorted(tf_vec.vocabulary_.items(), \n",
    "                     key=lambda item: item[1])])\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aahz</th>\n",
       "      <th>aan</th>\n",
       "      <th>aardvark</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>aba</th>\n",
       "      <th>aback</th>\n",
       "      <th>abacus</th>\n",
       "      <th>...</th>\n",
       "      <th>zweig</th>\n",
       "      <th>zweigniederlassung</th>\n",
       "      <th>zweimal</th>\n",
       "      <th>zweite</th>\n",
       "      <th>zweiten</th>\n",
       "      <th>zwinger</th>\n",
       "      <th>zwischen</th>\n",
       "      <th>zwischenmenschlichen</th>\n",
       "      <th>zyklus</th>\n",
       "      <th>zz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57505</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57506</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57507</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57508</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.092546</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57509</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57510 rows × 55258 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        aa  aaa  aahz  aan  aardvark     aaron   ab  aba  aback  abacus  ...  \\\n",
       "0      0.0  0.0   0.0  0.0       0.0  0.000000  0.0  0.0    0.0     0.0  ...   \n",
       "1      0.0  0.0   0.0  0.0       0.0  0.000000  0.0  0.0    0.0     0.0  ...   \n",
       "2      0.0  0.0   0.0  0.0       0.0  0.000000  0.0  0.0    0.0     0.0  ...   \n",
       "3      0.0  0.0   0.0  0.0       0.0  0.000000  0.0  0.0    0.0     0.0  ...   \n",
       "4      0.0  0.0   0.0  0.0       0.0  0.000000  0.0  0.0    0.0     0.0  ...   \n",
       "...    ...  ...   ...  ...       ...       ...  ...  ...    ...     ...  ...   \n",
       "57505  0.0  0.0   0.0  0.0       0.0  0.000000  0.0  0.0    0.0     0.0  ...   \n",
       "57506  0.0  0.0   0.0  0.0       0.0  0.000000  0.0  0.0    0.0     0.0  ...   \n",
       "57507  0.0  0.0   0.0  0.0       0.0  0.000000  0.0  0.0    0.0     0.0  ...   \n",
       "57508  0.0  0.0   0.0  0.0       0.0  0.092546  0.0  0.0    0.0     0.0  ...   \n",
       "57509  0.0  0.0   0.0  0.0       0.0  0.000000  0.0  0.0    0.0     0.0  ...   \n",
       "\n",
       "       zweig  zweigniederlassung  zweimal  zweite  zweiten  zwinger  zwischen  \\\n",
       "0        0.0                 0.0      0.0     0.0      0.0      0.0       0.0   \n",
       "1        0.0                 0.0      0.0     0.0      0.0      0.0       0.0   \n",
       "2        0.0                 0.0      0.0     0.0      0.0      0.0       0.0   \n",
       "3        0.0                 0.0      0.0     0.0      0.0      0.0       0.0   \n",
       "4        0.0                 0.0      0.0     0.0      0.0      0.0       0.0   \n",
       "...      ...                 ...      ...     ...      ...      ...       ...   \n",
       "57505    0.0                 0.0      0.0     0.0      0.0      0.0       0.0   \n",
       "57506    0.0                 0.0      0.0     0.0      0.0      0.0       0.0   \n",
       "57507    0.0                 0.0      0.0     0.0      0.0      0.0       0.0   \n",
       "57508    0.0                 0.0      0.0     0.0      0.0      0.0       0.0   \n",
       "57509    0.0                 0.0      0.0     0.0      0.0      0.0       0.0   \n",
       "\n",
       "       zwischenmenschlichen  zyklus   zz  \n",
       "0                       0.0     0.0  0.0  \n",
       "1                       0.0     0.0  0.0  \n",
       "2                       0.0     0.0  0.0  \n",
       "3                       0.0     0.0  0.0  \n",
       "4                       0.0     0.0  0.0  \n",
       "...                     ...     ...  ...  \n",
       "57505                   0.0     0.0  0.0  \n",
       "57506                   0.0     0.0  0.0  \n",
       "57507                   0.0     0.0  0.0  \n",
       "57508                   0.0     0.0  0.0  \n",
       "57509                   0.0     0.0  0.0  \n",
       "\n",
       "[57510 rows x 55258 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_search = input('What are you looking for: ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leopard vinyl essence thigh\n"
     ]
    }
   ],
   "source": [
    "print(book_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = preprocess_text(book_search)\n",
    "search_vec = tf_vec.transform([search])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = cosine_similarity(search_vec, vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title : The Vinyl Cafe Unplugged\n",
      "Author : Stuart McLean\n",
      "Publisher : Penguin Books\n",
      "Blurb : Why is Morley skulking around with a man named Frank on the eve of her 40th birthday? What grisly secret is Stephanie hiding in her father’s picnic cooler? And exactly what is Dave doing by himself in a Halifax hotel room with a duck? In the pages of the Vinyl Cafe Diaries, humorist Stuart McLean answers these questions and reveals more strange, shocking, and above all, entertaining truths about the seemingly ordinary folk of the Vinyl Cafe.\n"
     ]
    }
   ],
   "source": [
    "best_book = np.argmax(test[0])\n",
    "\n",
    "print('Title : {}'.format(df.loc[best_book, 'Title']))\n",
    "print('Author : {}'.format(df.loc[best_book, 'Author']))\n",
    "print('Publisher : {}'.format(df.loc[best_book, 'Publisher']))\n",
    "print('Blurb : {}'.format(df.loc[best_book, 'Blurb']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('tf-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a519badb918331f0d8c64ed18a42c37ad23f1f7898e2293b12d9d4e1cd8409d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
